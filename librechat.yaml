# For more information, see the Configuration Guide:
# https://docs.librechat.ai/install/configuration/custom_config.html

# Configuration version (required)
version: 1.0.9

# Cache settings: Set to true to enable caching
cache: true

# Custom interface configuration
interface:
  # Privacy policy settings
  privacyPolicy:
    externalUrl: 'https://librechat.ai/privacy-policy'
    openNewTab: true

  # Terms of service
  termsOfService:
    externalUrl: 'https://librechat.ai/tos'
    openNewTab: true

  endpointsMenu: true
  modelSelect: true
  parameters: true
  presets: true


# Example Registration Object Structure (optional)
registration:
  socialLogins: ['github', 'google', 'discord', 'openid', 'facebook']
  # allowedDomains:
  # - "gmail.com"

# rateLimits:
#   fileUploads:
#     ipMax: 100
#     ipWindowInMinutes: 60  # Rate limit window for file uploads per IP
#     userMax: 50
#     userWindowInMinutes: 60  # Rate limit window for file uploads per user
#   conversationsImport:
#     ipMax: 100
#     ipWindowInMinutes: 60  # Rate limit window for conversation imports per IP
#     userMax: 50
#     userWindowInMinutes: 60  # Rate limit window for conversation imports per user

# Definition of custom endpoints
endpoints:
  # assistants:
  #   disableBuilder: false # Disable Assistants Builder Interface by setting to `true`
  #   pollIntervalMs: 750  # Polling interval for checking assistant updates
  #   timeoutMs: 180000  # Timeout for assistant operations
  #   # Should only be one or the other, either `supportedIds` or `excludedIds`
  #   supportedIds: ["asst_supportedAssistantId1", "asst_supportedAssistantId2"]
  #   # excludedIds: ["asst_excludedAssistantId"]
  #   # (optional) Models that support retrieval, will default to latest known OpenAI models that support the feature
  #   retrievalModels: ["gpt-4-turbo-preview"]
  #   # (optional) Assistant Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.
  #   capabilities: ["code_interpreter", "retrieval", "actions", "tools", "image_vision"]
  custom:
    # Groq Example
    - name: 'groq'
      apiKey: '${GROQ_API_KEY}'
      # apiKey: 'user_provided'
      baseURL: 'https://api.groq.com/openai/v1/'
      models:
        default: [
          "llama3-70b-8192",
          "llama3-8b-8192",
          "llama2-70b-4096",
          "mixtral-8x7b-32768",
          "gemma-7b-it",
          ]
        fetch: true
      titleConvo: true
      titleModel: 'mixtral-8x7b-32768'
      modelDisplayLabel: 'groq'

    # DeepSeek
    - name: 'deepseek'
      apiKey: '${DEEPSEEK_API_KEY}'
      # apiKey: 'user_provided'
      baseURL: 'https://api.deepseek.com/'
      models:
        default: [
          "deepseek-coder",
          "deepseek-chat",
          ]
        fetch: true
      titleConvo: true
      titleModel: 'deepseek-coder'
      modelDisplayLabel: 'deepseek'
      iconURL: '/assets/deepseek.svg'

    # Custom Workflows
    - name: 'Custom Workflows'
      apiKey: 'not_needed'
      # apiKey: 'user_provided'
      baseURL: 'http://custom_workflows:8000'
      models:
        default: [
          "llama-3-8b-groq-basic",
          ]
        fetch: true
      titleConvo: true
      titleModel: 'llama-3-8b-groq-basic'
      modelDisplayLabel: 'Custom AI'
      # dropParams: ['stop']
      iconURL: '/assets/nixlab_nobg.svg'

    # Mistral AI Example
    - name: 'Mistral' # Unique name for the endpoint
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      # apiKey: '${MISTRAL_API_KEY}'
      apiKey: 'user_provided'
      baseURL: 'https://api.mistral.ai/v1'

      # Models configuration
      models:
        # List of default models to use. At least one value is required.
        default: ['mistral-tiny', 'mistral-small', 'mistral-medium']
        # Fetch option: Set to true to fetch models from API.
        fetch: true # Defaults to false.

      # Optional configurations

      # Title Conversation setting
      titleConvo: true # Set to true to enable title conversation

      # Title Method: Choose between "completion" or "functions".
      # titleMethod: "completion"  # Defaults to "completion" if omitted.

      # Title Model: Specify the model to use for titles.
      titleModel: 'mistral-tiny' # Defaults to "gpt-3.5-turbo" if omitted.

      # Summarize setting: Set to true to enable summarization.
      # summarize: false

      # Summary Model: Specify the model to use if summarization is enabled.
      # summaryModel: "mistral-tiny"  # Defaults to "gpt-3.5-turbo" if omitted.

      # Force Prompt setting: If true, sends a `prompt` parameter instead of `messages`.
      # forcePrompt: false

      # The label displayed for the AI model in messages.
      modelDisplayLabel: 'Mistral' # Default is "AI" when not set.

      # Add additional parameters to the request. Default params will be overwritten.
      # addParams:
      # safe_prompt: true # This field is specific to Mistral AI: https://docs.mistral.ai/api/

      # Drop Default params parameters from the request. See default params in guide linked below.
      # NOTE: For Mistral, it is necessary to drop the following parameters or you will encounter a 422 Error:
      dropParams: ['stop', 'user', 'frequency_penalty', 'presence_penalty']

    # OpenRouter Example
    - name: 'OpenRouter'
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      # Known issue: you should not use `OPENROUTER_API_KEY` as it will then override the `openAI` endpoint to use OpenRouter as well.
      apiKey: '${OPENROUTER_KEY}'
      # apiKey: 'user_provided'
      baseURL: 'https://openrouter.ai/api/v1'
      models:
      # see `curl https://openrouter.ai/api/v1/models |  jq` for available models
        default: [
          "openrouter/auto"
          ,"mistralai/mistral-7b-instruct:free"
          ,"openchat/openchat-7b:free"
          ,"google/gemma-7b-it:free"
          ,"meta-llama/llama-3-8b-instruct:free"
          ,"meta-llama/llama-3-8b-instruct:extended"
          ,"meta-llama/llama-3-8b-instruct"
          ,"meta-llama/llama-3-70b-instruct"
          ,"open-orca/mistral-7b-openorca"
          ,"mistralai/mixtral-8x7b"
          ,"mistralai/mixtral-8x7b-instruct:nitro"
          ,"mistralai/mixtral-8x7b-instruct"
          ,"mistralai/mistral-7b-instruct:nitro"
          ,"mistralai/mistral-tiny"
          ,"mistralai/mistral-small"
          ,"mistralai/mistral-medium"
          ,"mistralai/mistral-large"
          ,"openai/gpt-3.5-turbo"
          ,"openai/gpt-3.5-turbo-0125"
          ,"openai/gpt-3.5-turbo-1106"
          ,"openai/gpt-3.5-turbo-0613"
          ,"openai/gpt-3.5-turbo-0301"
          ,"openai/gpt-3.5-turbo-16k"
          ,"openai/gpt-4-turbo"
          ,"openai/gpt-4-turbo-preview"
          ,"openai/gpt-4-1106-preview"
          ,"openai/gpt-4"
          ,"openai/gpt-4-0314"
          ,"openai/gpt-4-32k"
          ,"openai/gpt-4-32k-0314"
          ,"openai/gpt-4-vision-preview"
          ,"openai/gpt-3.5-turbo-instruct"
          ,"google/palm-2-chat-bison"
          ,"google/palm-2-codechat-bison"
          ,"google/palm-2-chat-bison-32k"
          ,"google/palm-2-codechat-bison-32k"
          ,"google/gemini-pro"
          ,"google/gemini-pro-vision"
          ,"google/gemini-pro-1.5"
          ,"perplexity/pplx-70b-online"
          ,"perplexity/pplx-7b-online"
          ,"perplexity/pplx-7b-chat"
          ,"perplexity/pplx-70b-chat"
          ,"perplexity/sonar-small-chat"
          ,"perplexity/sonar-medium-chat"
          ,"perplexity/sonar-small-online"
          ,"perplexity/sonar-medium-online"
          ,"fireworks/firellava-13b"
          ,"anthropic/claude-3-opus"
          ,"anthropic/claude-3-sonnet"
          ,"anthropic/claude-3-haiku"
          ,"anthropic/claude-2"
          ,"anthropic/claude-2.0"
          ,"anthropic/claude-2.1"
          ,"anthropic/claude-instant-1"
          ,"anthropic/claude-3-opus:beta"
          ,"anthropic/claude-3-sonnet:beta"
          ,"anthropic/claude-3-haiku:beta"
          ,"anthropic/claude-2:beta"
          ,"anthropic/claude-2.0:beta"
          ,"anthropic/claude-2.1:beta"
          ,"anthropic/claude-instant-1:beta"
          ,"google/gemma-7b-it"
          ,"databricks/dbrx-instruct"
          ,"huggingfaceh4/zephyr-7b-beta:free"
          ,"google/gemma-7b-it:nitro"
          ,"undi95/toppy-m-7b:nitro"
          ,"microsoft/wizardlm-2-8x22b:nitro"
          ,"meta-llama/llama-3-8b-instruct:nitro"
          ,"meta-llama/llama-3-70b-instruct:nitro"
          ,"haotian-liu/llava-13b"
          ,"nousresearch/nous-hermes-2-vision-7b"
          ,"cohere/command"
          ,"cohere/command-r"
          ,"cohere/command-r-plus"
        ]
        fetch: true
      titleConvo: true
      titleModel: 'meta-llama/llama-3-8b-instruct:free'
      # titleModel: 'meta-llama/llama-3-70b-instruct'
      # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.
      dropParams: ['stop']
      modelDisplayLabel: 'OpenRouter'

modelSpecs:
  enforce: false
  prioritize: false
  list:

# From DeepSeek
    - name: "deepseek-coder-generic"
      label: "DeepSeek Coder"
      description: "Coding and IT expert"
      preset:
        endpoint: "deepseek"
        model: "deepseek-coder"
        modelLabel: "DeepSeek Coder"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: ""
        # model options
        temperature: 0.5
        top_p: 0.6
        top_k: 35
        frequency_penalty: 0
        presence_penalty: 0


    - name: "deepseek-coder-solid"
      label: "DeepSeek SOLID Coder"
      description: "Coding and IT expert"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/deepseek-solid.gif"
      preset:
        endpoint: "deepseek"
        model: "deepseek-coder"
        modelLabel: "DeepSeek SOLID coder"
        promptPrefix: "Follow the instruction or continue the conversation. Remember to write clean, highly optimal, safe and readable code that follows SOLID rules. Output just the code with no comments. Add no or minimal explanation unless asked. Instruction:\n"
        greeting: "Hello, how can I assist you today?"
        # model options
        temperature: 0.5
        top_p: 0.5
        top_k: 35
        frequency_penalty: 0
        presence_penalty: 0


# From Groq
    - name: "groq-mixtral-8x7b"
      label: "Mixtral 8x7B"
      description: "Mixtral 8x7B 32768 by Groq"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/mistral.gif"
      preset:
        endpoint: "groq"
        modelLabel: "Mixtral 8x7B"
        model: "mixtral-8x7b-32768"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: ""
        # model options
        temperature: 0.8
        top_p: 0.7
        top_k: 50
        frequency_penalty: 0
        presence_penalty: 0

    - name: "groq-llama3-70b"
      label: "Llama 3 70B"
      description: "Llama 3 70B 8192 by Groq"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/meta-llama-3.gif"
      preset:
        endpoint: "groq"
        model: "llama3-70b-8192"
        modelLabel: "Llama 3 70B"
        promptPrefix: ""

        greeting: "Hello, how can I assist you today?"
        # model options
        temperature: 0.8
        top_p: 0.7
        top_k: 50
        frequency_penalty: 0
        presence_penalty: 0

    - name: "groq-solid-llama3-70b"
      label: "Llama 3 70B SOLID Coder"
      description: "Llama 3 70B coding assistant powered by Groq"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/llama-solid.gif"
      preset:
        endpoint: "groq"
        model: "llama3-70b-8192"
        modelLabel: "Llama 3 SOLID Coder"
        promptPrefix: "Follow the instruction or continue the conversation. Remember to write clean, highly optimal, safe and readable code that follows SOLID rules. Output just the code with no comments. Add no or minimal explanation unless asked. Instruction:\n"

        greeting: "Hello, how can I assist you today?"
        # model options
        temperature: 0.5
        top_p: 0.7
        top_k: 35
        frequency_penalty: 0
        presence_penalty: 0

    - name: "groq-crazy-llama3-70b"
      label: "Llama 3 70B Creative assistant"
      description: "Llama 3 70B creative assistant powered by Groq"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/crazy-llama.gif"
      preset:
        endpoint: "groq"
        model: "llama3-70b-8192"
        modelLabel: "Creative and innovative Llama 3"
        promptPrefix: "Follow the instruction or continue the conversation. Be very creative and unique. Think out of the box. You can be funny sometimes. Instruction:\n"
        greeting: "Hello, what are we going to create today?"
        # model options
        temperature: 1.5
        top_p: 1
        top_k: 60
        frequency_penalty: 0
        presence_penalty: 0


 # From OpenRouter
    - name: "ort-claude-3-opus"
      label: "Claude 3 Opus"
      description: "Claude 3 Opus from Anthropic"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/claude.gif"
      preset:
        endpoint: "OpenRouter"
        model: "anthropic/claude-3-opus"
        modelLabel: "Claude 3 Opus"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: ""
        # model options
        temperature: 0.6
        top_p: 0.7
        top_k: 50
        frequency_penalty: 0
        presence_penalty: 0

    - name: "ort-claude-3-opus-solid"
      label: "Claude 3 Opus SOLID Coder"
      description: "Claude 3 Opus that follows SOLID rules"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/opus-coder.gif"
      preset:
        endpoint: "OpenRouter"
        model: "anthropic/claude-3-opus"
        modelLabel: "Claude 3 Opus SOLID coder"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: "Follow the instruction or continue the conversation. Remember to write clean, highly optimal, safe and readable code that follows SOLID rules. Output just the code with no comments. Add no or minimal explanation unless asked. Instruction:\n"
        # model options
        temperature: 0.5
        top_p: 0.5
        top_k: 35
        frequency_penalty: 0
        presence_penalty: 0

    - name: "ort-gpt-4o"
      label: "GPT-4o"
      description: "GPT-4o from OpenAI"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/gpt-4.gif"
      preset:
        endpoint: "OpenRouter"
        model: "openai/gpt-4o"
        modelLabel: "GPT-4o"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: ""
        # model options
        temperature: 0.8
        top_p: 0.7
        top_k: 50
        frequency_penalty: 0
        presence_penalty: 0

    - name: "ort-gpt-4-turbo"
      label: "GPT-4 Turbo"
      description: "GPT-4 Turbo from OpenAI"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/gpt-4.gif"
      preset:
        endpoint: "OpenRouter"
        model: "openai/gpt-4-turbo"
        modelLabel: "GPT-4 Turbo"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: ""
        # model options
        temperature: 0.8
        top_p: 0.7
        top_k: 50
        frequency_penalty: 0
        presence_penalty: 0

    - name: "ort-gpt-3.5-turbo"
      label: "GPT-3.5 Turbo"
      description: "GPT-3.5 Turbo from OpenAI"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/gpt-3.5.gif"
      preset:
        endpoint: "OpenRouter"
        model: "openai/gpt-3.5-turbo"
        modelLabel: "GPT-3.5 Turbo"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: ""
        # model options
        temperature: 0.7
        top_p: 0.7
        top_k: 50
        frequency_penalty: 0
        presence_penalty: 0

    - name: "ort-gemini-1.5-pro"
      label: "Gemini 1.5 Pro"
      description: "Gemini 1.5 Pro (preview) from Google"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/google-gemini.gif"
      preset:
        endpoint: "OpenRouter"
        model: "google/gemini-pro-1.5"
        modelLabel: "Gemini 1.5 Pro"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: ""
        # model options
        temperature: 0.8
        top_p: 0.7
        top_k: 50
        frequency_penalty: 0
        presence_penalty: 0

    - name: "ort-llama-3-70b-instruct"
      label: "Llama 3 70B Instruct"
      description: "Llama 3 70B Instruct from Meta"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/meta-llama-3.gif"
      preset:
        endpoint: "OpenRouter"
        model: "meta-llama/llama-3-70b-instruct"
        modelLabel: "Llama 3 70B Instruct"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: ""
        # model options
        temperature: 0.8
        top_p: 0.7
        top_k: 50
        frequency_penalty: 0
        presence_penalty: 0

    - name: "ort-mixtral-8x22b-instruct"
      label: "Mixtral 8x22B Instruct"
      description: "Mixtral 8x22B Instruct from Mistral AI"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/mistral.gif"
      preset:
        endpoint: "OpenRouter"
        model: "mistralai/mixtral-8x22b-instruct"
        modelLabel: "Mixtral 8x22B Instruct"
        greeting: "Hello, how can I assist you today?"
        promptPrefix: ""
        # model options
        temperature: 0.8
        top_p: 0.7
        top_k: 50
        frequency_penalty: 0
        presence_penalty: 0

 # From local proxy (python workflows)
    - name: "cw-echo-test-model"
      label: "Dummy workflow model"
      description: "Just for testing"
      preset:
        endpoint: "Custom Workflows"
        name: "echo-test-model"
        modelLabel: "Dummy workflow model"
        greeting: "Hello, I'm a dummy model"
        promptPrefix: ""

    - name: "coding-duo"
      label: "Coding Duo"
      description: "Two-model code generation"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/llama-deepseek-01.gif"
      preset:
        endpoint: "Custom Workflows"
        model: "coding-duo"
        modelLabel: "Coding Duo"
        greeting: "Hi! Let's code together!"
        promptPrefix: ""

    - name: "senior-coding-duo"
      label: "Senior Coding Duo"
      description: "Two-model code generation"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/coding-duo-02.gif"
      preset:
        endpoint: "Custom Workflows"
        model: "senior-coding-duo"
        modelLabel: "Senior Coding Duo"
        greeting: "Hi! Let's code together with seniors! Be aware - we are expensive :)"
        promptPrefix: ""

    - name: "llama-3-70b-code-saver"
      label: "Llama Code Saver"
      description: "Workflow with code saving"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/llama-saver.gif"
      preset:
        endpoint: "Custom Workflows"
        model: "llama-3-70b-code-saver"
        modelLabel: "Llama Code Saver"
        greeting: "Hello, what can I do for you today?"
        promptPrefix: ""

    - name: "deepseek-code-saver"
      label: "DeepSeek Code Saver"
      description: "Workflow with code saving"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/deepseek-saver.gif"
      preset:
        endpoint: "Custom Workflows"
        model: "deepseek-code-saver"
        modelLabel: "DeepSeek Code Saver"
        greeting: "Hello, what can I do for you today?"
        promptPrefix: ""

    - name: "gpt4-code-saver"
      label: "GPT-4o Code Saver"
      description: "Workflow with code saving"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/gpt-saver.gif"
      preset:
        endpoint: "Custom Workflows"
        model: "gpt-4-code-saver"
        modelLabel: "GPT-4o Code Saver"
        greeting: "Hello, what can I do for you today?"
        promptPrefix: ""

    - name: "llama-code-executor"
      label: "Llama Code Executor"
      description: "Workflow with code saving"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/llama-executor.gif"
      preset:
        endpoint: "Custom Workflows"
        model: "llama-code-executor"
        modelLabel: "Llama Code Executor"
        greeting: "Hello, what can I do for you today?"
        promptPrefix: ""

    - name: "gpt4-code-executor"
      label: "GPT-4o Code Executor"
      description: "Workflow with code saving"
      iconURL: "${env:AI_WORKFLOWS_WEB_BASE}/img/icons/gpt-executor.gif"
      preset:
        endpoint: "Custom Workflows"
        model: "gpt-4-code-executor"
        modelLabel: "GPT-4o Code Executor"
        greeting: "Hello, what can I do for you today?"
        promptPrefix: ""


# fileConfig:
#   endpoints:
#     assistants:
#       fileLimit: 5
#       fileSizeLimit: 10  # Maximum size for an individual file in MB
#       totalSizeLimit: 50  # Maximum total size for all files in a single request in MB
#       supportedMimeTypes:
#         - "image/.*"
#         - "application/pdf"
#     openAI:
#       disabled: true  # Disables file uploading to the OpenAI endpoint
#     default:
#       totalSizeLimit: 20
#     YourCustomEndpointName:
#       fileLimit: 2
#       fileSizeLimit: 5
#   serverFileSizeLimit: 100  # Global server file size limit in MB
#   avatarSizeLimit: 2  # Limit for user avatar image size in MB
# See the Custom Configuration Guide for more information:
# https://docs.librechat.ai/install/configuration/custom_config.html
